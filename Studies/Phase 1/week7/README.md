# Week 7: 최소자승법 (Least Squares)

## 📌 개요

> 🎯 **목표**: 오차 최소화를 통한 최적 추정 이해하기
> ⏱️ **예상 시간**: 이론 2시간 + 실습 2시간

**최소자승법(Least Squares)** 은 SLAM 최적화의 **가장 기본적인 형태**입니다. 센서 측정값과 예측값 사이의 오차를 최소화하여 최적의 추정치를 찾습니다.

### 🤔 왜 이걸 배워야 할까요?

**일상 비유**: 과녁 맞추기

```
여러 발 쏜 화살들:
        ⊗
      ⊗   ⊗
    ──⊗──●──⊗──  ← 과녁 중심
      ⊗   ⊗
        ⊗

"모든 화살과 가장 가까운 점은 어디인가?"
→ 오차 제곱합을 최소화하는 점 찾기!
```

**SLAM에서의 중요성**:
- **센서 노이즈**: 측정은 항상 불완전 → 여러 측정으로 추정
- **과결정 시스템**: 방정식이 미지수보다 많음 → 정확한 해 없음
- **모든 SLAM 최적화의 기초**: PnP, 삼각측량, Bundle Adjustment

---

## 📖 핵심 개념

### 1. 최소자승 문제 정의

#### 문제 상황

```
GPS 위치 측정 예시:

측정 1: (3.1, 4.2)   ┌─────────────────┐
측정 2: (2.9, 3.8)   │     ⊗1          │
측정 3: (3.2, 4.1)   │   ⊗2  ●?  ⊗3    │ ← 진짜 위치는?
측정 4: (2.8, 4.0)   │     ⊗4          │
                    └─────────────────┘

→ 4개 측정값에서 1개 위치 추정
→ "과결정" 시스템!
```

#### 수학적 표현

```
Ax = b

A: m×n 설계 행렬 (m개 방정식, n개 미지수)
x: n×1 미지수 벡터 (구하고자 하는 값!)
b: m×1 관측 벡터 (측정값)

과결정: m > n (방정식 > 미지수)
→ 정확한 해 없음!
→ 대신 "가장 좋은" 해 찾기
```

#### 최소자승 목표

**오차의 제곱합 최소화**:

```
min ||Ax - b||²
 x
      ↑
   잔차(residual)의 L2 norm(벡터 크기, 노름)
```

**왜 제곱?**
- 음수 오차와 양수 오차가 상쇄되지 않음
- 미분이 쉬움 (최소점 찾기 용이)
- 큰 오차에 더 큰 페널티

---

### 2. 잔차 (Residual)

#### 정의

```
r = b - Ax
    ↑     ↑
 측정값  예측값

잔차 = 측정값 - 모델이 예측한 값
```

**비유**: 과녁에서 얼마나 빗나갔는지

```
        실제 값 (b)
           ⬇
         ⊗───────●  예측값 (Ax)
              ↑
           잔차 (r)
```

#### 비용 함수

```
J(x) = ||r||² = ||b - Ax||² = Σ rᵢ²

→ 모든 잔차 제곱의 합
→ 이것을 최소화!
```

> **⚠️ 용어 주의: J(x) vs Jacobian**
>
> **J(x)** (여기서): **비용 함수 (Cost Function)**
> - 스칼라 값 (하나의 숫자)
> - 최소화하고자 하는 대상
>
> **J** (Week 8에서): **자코비안 행렬 (Jacobian Matrix)**
> - 행렬 (벡터 함수의 1차 편미분들)
> - `J = ∂r/∂x` (잔차를 변수로 미분)
> - 비용 함수를 최소화하는 **도구**
>
> 같은 문자 **J**를 사용하지만 전혀 다른 개념!
> - 비용 함수 J(x): "무엇을" 최소화할 것인가
> - 자코비안 J: "어떻게" 최소화할 것인가

---

### 3. 정규방정식 (Normal Equation)

#### 유도 과정

비용 함수를 x로 미분하여 0으로 설정:

```
J(x) = ||Ax - b||² = (Ax - b)ᵀ(Ax - b)
     = xᵀAᵀAx - 2bᵀAx + bᵀb

∂J/∂x = 2AᵀAx - 2Aᵀb = 0
```

#### 유도 과정 상세 설명

> **📘 왜 `||Ax - b||²` = `(Ax - b)ᵀ(Ax - b)`인가?**
>
> **노름의 정의:**
> ```
> 벡터 v의 L2 노름:
> ||v||² = v₁² + v₂² + ... + vₙ²
> 
> 전치행렬 곱으로 표현:
> ||v||² = vᵀv = [v₁ v₂ v₃] · [v₁]
>                              [v₂] = v₁² + v₂² + v₃²
>                              [v₃]
> ```
>
> **차원 분석:**
> ```
> Ax - b: m×1 벡터 (열벡터)
> 
> 직접 곱셈 불가:
>   (Ax - b) × (Ax - b)
>     m×1        m×1     ← 차원 불일치! ❌
> 
> 전치 후 곱셈 가능:
>   (Ax - b)ᵀ × (Ax - b)
>     1×m         m×1    = 1×1 (스칼라) ✅
> ```
>
> **전치의 목적**: 열벡터를 행벡터로 바꿔 행렬 곱셈 차원을 맞춤
> - **결과**: 내적(inner product) = 노름 제곱

> **📘 `A²` vs `AᵀA` 차이점**
>
> | 표현 | 의미 | 조건 | 차원 |
> |------|------|------|------|
> | `A²` | A × A | A가 정사각행렬(n×n)만 가능 | n×n |
> | `AᵀA` | Aᵀ × A | 모든 행렬 가능 | **항상 n×n** |
>
> **최소자승에서 `AᵀA`가 나타나는 이유:**
> ```
> (Ax - b)ᵀ(Ax - b) 전개 시 자연스럽게 등장
> 
> = (xᵀAᵀ - bᵀ)(Ax - b)
> = xᵀAᵀAx - xᵀAᵀb - bᵀAx + bᵀb
>     └┬┘      └─┬─┘   └┬┘
>    여기!      여기!   여기!
> ```
>
> **핵심**: `AᵀA`는 "A의 제곱"이 아니라, **전개 과정의 결과**
> - A가 m×n (m > n) 비정사각행렬이어도 AᵀA는 n×n 정사각행렬
> - 항상 대칭행렬(Symmetric Matrix)
> - 그람 행렬(Gram Matrix)로도 불림

#### 정규방정식

```
AᵀA · x = Aᵀb
  ↑          ↑
n×n 행렬   n×1 벡터

해: x = (AᵀA)⁻¹ Aᵀb
```

**기하학적 의미**:

```
열 공간 (A의 열 벡터가 만드는 공간)

           b (측정값)
          /│
         / │ ← 잔차 (수직!)
        /  │
       /   │
    ──────●─────── A의 열 공간
          Ax̂ (최적 예측)

Ax̂ = b를 A의 열 공간에 "수직 투영"!
```

---

### 4. 풀이 방법 비교

#### 방법 1: 직접 역행렬

```python
x = np.linalg.inv(A.T @ A) @ A.T @ b
```

⚠️ 수치적으로 불안정 (작은 고유값 문제)

#### 방법 2: solve 사용

```python
x = np.linalg.solve(A.T @ A, A.T @ b)
```

✅ 역행렬보다 안정적

#### 방법 3: SVD (가장 안정)

```python
x = np.linalg.lstsq(A, b, rcond=None)[0]
```

✅ 내부적으로 SVD 사용, 가장 권장

#### 비교표

| 방법 | 안정성 | 속도 | 권장 |
|------|--------|------|------|
| inv + @ | 낮음 | 빠름 | ❌ |
| solve | 중간 | 빠름 | ○ |
| lstsq/SVD | 높음 | 중간 | **✅** |

---

### 5. 직선 피팅 예제

#### 문제

```
데이터: (x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)
모델: y = ax + b (직선)
목표: a, b 찾기
```

#### 설계 행렬 구성

```
y = ax + b

   y₁ = a·x₁ + b·1       ⎡ x₁  1 ⎤   ⎡a⎤   ⎡y₁⎤
   y₂ = a·x₂ + b·1   →   ⎢ x₂  1 ⎥ · ⎢ ⎥ = ⎢y₂⎥
   ...                   ⎢ ...   ⎥   ⎣b⎦   ⎢..⎥
   yₙ = a·xₙ + b·1        ⎣ xₙ  1 ⎦         ⎣yₙ⎦
                              A        x     b
```

#### 코드

```python
import numpy as np

# 데이터
x_data = np.array([1, 2, 3, 4, 5])
y_data = np.array([2.8, 5.1, 7.2, 9.1, 10.9])  # y ≈ 2x + 1

# 설계 행렬
A = np.column_stack([x_data, np.ones(5)])

# 최소자승 해
params = np.linalg.lstsq(A, y_data, rcond=None)[0]
a, b = params

print(f"피팅 결과: y = {a:.2f}x + {b:.2f}")
# 출력: y = 2.01x + 0.82
```

---

### 6. SLAM에서의 활용

#### 문제별 형태

```
┌─────────────────────────────────────────────────────┐
│                    SLAM 문제들                        │
├─────────────────────────────────────────────────────┤
│                                                     │
│  PnP (3D→2D)        min Σ||π(T·P) - p||²            │
│                          투영      관측               │
│                                                     │
│  Triangulation      A·X = 0  (동차 시스템)             │
│  (2D+2D→3D)        SVD로 null space 찾기              │
│                                                     │
│  Line Fitting       A·[a,b]ᵀ = y                    │
│                                                     │
│  Bundle Adjustment  매우 큰 최소자승!                   │
│                     포즈 + 3D점 동시 최적화              │
│                                                     │
└─────────────────────────────────────────────────────┘
```

#### 선형 vs 비선형

```
선형 최소자승:              비선형 최소자승:
  A·x = b                    f(x) = z
  직접 풀 수 있음             반복적 선형화 필요
  (정규방정식)                (Gauss-Newton, LM)
                              → Week 8에서!
```

---

## 💻 실습 파일

| 파일 | 내용 | 난이도 |
|------|------|--------|
| `least_squares_basics.py` | 정규방정식, 직선 피팅 | ⭐⭐ |
| `least_squares_quiz.py` | SVD, 위치 추정 | ⭐⭐ |

---

## 📊 핵심 정리

### 공식 요약

| 항목 | 공식 |
|------|------|
| 잔차 | r = b - Ax |
| 비용 함수 | J(x) = \|\|r\|\|² = Σrᵢ² |
| 정규방정식 | AᵀAx = Aᵀb |
| 최소자승 해 | x = (AᵀA)⁻¹Aᵀb |

### 핵심 포인트

```
1. 과결정 시스템: 방정식 > 미지수
2. 정확한 해 없음 → "최선의" 해
3. 잔차 제곱합 최소화
4. 정규방정식으로 해석적 해
5. NumPy lstsq가 가장 실용적
```

---

## ✅ 학습 완료 체크리스트

### 기초 이해 (필수)
- [ ] 과결정 시스템이 왜 정확한 해가 없는지 설명 가능
- [ ] 잔차의 의미 설명 가능
- [ ] 정규방정식 유도 과정 이해

### 실용 활용 (권장)
- [ ] np.linalg.lstsq() 사용 가능
- [ ] 직선 피팅 코드 작성 가능
- [ ] 잔차 계산 및 분석 가능

### 심화 (선택)
- [ ] SVD로 최소자승 해 유도 이해
- [ ] AᵀA의 기하학적 의미 이해
- [ ] 가중 최소자승 개념 이해

---

## 🔗 다음 단계

### Week 8: 비선형 최적화

실제 SLAM 문제는 대부분 비선형:
- **Gauss-Newton**: 반복적 선형화
- **Levenberg-Marquardt**: 안정적 수렴
- **Ceres Solver**: 실전 라이브러리

---

## 📚 참고 자료

- Linear Algebra (Strang) - Chapter 4
- State Estimation for Robotics (Barfoot)
- NumPy documentation: linalg.lstsq

---

## ❓ FAQ

**Q1: 왜 절대값이 아니라 제곱인가요?**
A: 제곱이 미분이 쉽고, 큰 오차에 더 큰 페널티. 절대값은 미분 불연속.

**Q2: AᵀA가 역행렬이 안되면?**
A: 행렬이 rank-deficient한 경우. SVD 사용하거나 정규화(regularization) 추가.

**Q3: 잔차 vs 오차?**
A: 잔차(residual)는 측정-예측. 오차(error)는 추정-진짜값. 진짜값을 보통 모르므로 잔차 사용.

**Q4: SLAM에서 언제 선형 최소자승을 쓰나요?**
A: 초기화(삼각측량), 선형 근사가 가능할 때. 대부분은 비선형.

---

**🎯 Week 7 핵심 메시지:**

> 최소자승 = 오차 제곱합 최소화
>
> **AᵀAx = Aᵀb** 하나면 끝!
> SLAM 최적화의 모든 기초!
