# Week 8: ë¹„ì„ í˜• ìµœì í™” (Nonlinear Optimization)

## ğŸ“Œ ê°œìš”

ì‹¤ì œ SLAM ë¬¸ì œëŠ” ëŒ€ë¶€ë¶„ **ë¹„ì„ í˜•**ì…ë‹ˆë‹¤. ì¹´ë©”ë¼ íˆ¬ì˜, íšŒì „ ì—°ì‚° ë“±ì´ ëª¨ë‘ ë¹„ì„ í˜•ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ë²ˆ ì£¼ì—ëŠ” **Gauss-Newton**, **Levenberg-Marquardt** ì•Œê³ ë¦¬ì¦˜ì„ í•™ìŠµí•˜ê³ , SLAMì—ì„œ ì‹¤ì œë¡œ ì‚¬ìš©ë˜ëŠ” **Ceres Solver**ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤.

## ğŸ¯ í•™ìŠµ ëª©í‘œ

1. ë¹„ì„ í˜• ë¬¸ì œì˜ ì„ í˜•í™” ì´í•´
2. Jacobian í–‰ë ¬ì˜ ì˜ë¯¸
3. Gauss-Newton ì•Œê³ ë¦¬ì¦˜
4. Levenberg-Marquardt ì•Œê³ ë¦¬ì¦˜
5. Ceres Solver ê¸°ë³¸ ì‚¬ìš©ë²•

## â±ï¸ ì˜ˆìƒ í•™ìŠµ ì‹œê°„: **5-7ì‹œê°„**

---

## ğŸ“– í•µì‹¬ ê°œë…

### 1. ë¹„ì„ í˜• ìµœì†ŒììŠ¹

```
min Î£ ||f(x) - z||Â²
 x

f(x): ë¹„ì„ í˜• í•¨ìˆ˜
z: ì¸¡ì •ê°’
```

ì„ í˜•í™” (1ì°¨ Taylor ì „ê°œ):
```
f(x + Î”x) â‰ˆ f(x) + JÂ·Î”x

J = âˆ‚f/âˆ‚x (Jacobian í–‰ë ¬)
```

### 2. Gauss-Newton

```
ë°˜ë³µ:
  1. Jacobian J = âˆ‚f/âˆ‚x ê³„ì‚°
  2. ì”ì°¨ r = z - f(x) ê³„ì‚°
  3. ì •ê·œë°©ì •ì‹: Jáµ€JÂ·Î”x = Jáµ€r
  4. ì—…ë°ì´íŠ¸: x â† x + Î”x
```

```python
def gauss_newton(f, J_func, x0, z, max_iter=10):
    x = x0.copy()
    for i in range(max_iter):
        J = J_func(x)
        r = z - f(x)
        dx = np.linalg.solve(J.T @ J, J.T @ r)
        x = x + dx
        if np.linalg.norm(dx) < 1e-8:
            break
    return x
```

### 3. Levenberg-Marquardt

Gauss-Newtonì˜ ë¬¸ì œ: ì´ˆê¸°ê°’ì´ ë©€ë©´ ë°œì‚°

LM í•´ê²°ì±…:
```
(Jáµ€J + Î»I)Â·Î”x = Jáµ€r

Î» í¬ë©´ â†’ Gradient Descent (ì•ˆì •ì )
Î» ì‘ìœ¼ë©´ â†’ Gauss-Newton (ë¹ ë¦„)
```

---

## ğŸ”§ Ceres Solver

### C++ ì˜ˆì œ (ê³¡ì„  í”¼íŒ…)

```cpp
struct CostFunctor {
    CostFunctor(double x, double y) : x_(x), y_(y) {}
    
    template <typename T>
    bool operator()(const T* const params, T* residual) const {
        // y = a*exp(b*x)
        residual[0] = y_ - params[0] * ceres::exp(params[1] * x_);
        return true;
    }
    
private:
    const double x_, y_;
};

// Problem ì„¤ì •
ceres::Problem problem;
for (int i = 0; i < N; ++i) {
    problem.AddResidualBlock(
        new ceres::AutoDiffCostFunction<CostFunctor, 1, 2>(
            new CostFunctor(x[i], y[i])),
        nullptr, params);
}

// ìµœì í™” ì‹¤í–‰
ceres::Solve(options, &problem, &summary);
```

### Python (scipy)

```python
from scipy.optimize import least_squares

def residual(params, x, y):
    a, b = params
    return y - a * np.exp(b * x)

result = least_squares(residual, x0=[1, 0], args=(x_data, y_data))
```

---

## ğŸ¤– SLAMì—ì„œì˜ í™œìš©

| ë¬¸ì œ | Ceres ì—­í•  |
|------|-----------|
| **Bundle Adjustment** | í¬ì¦ˆ + 3Dì  ë™ì‹œ ìµœì í™” |
| **VIO** | IMU ì‚¬ì „ì ë¶„ ìµœì í™” |
| **PnP Refinement** | ì´ˆê¸° PnP í•´ ì •ì œ |

### VINS-Fusionì—ì„œ

```cpp
ceres::Problem problem;

// ì¹´ë©”ë¼ í¬ì¦ˆ ìµœì í™”
for (auto& frame : keyframes) {
    problem.AddParameterBlock(frame.pose, 7, new PoseLocalParameterization());
}

// ì¬íˆ¬ì˜ ì˜¤ì°¨
for (auto& obs : observations) {
    problem.AddResidualBlock(
        new ReprojectionError(obs),
        loss_function,
        frame_pose, point_3d);
}
```

---

## ğŸ’» ì‹¤ìŠµ íŒŒì¼

| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `nonlinear_basics.py` | Gauss-Newton, ê³¡ì„  í”¼íŒ… |
| `nonlinear_quiz.py` | ê°œë… í€´ì¦ˆ |

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ë¹„ì„ í˜• ë¬¸ì œì˜ ì„ í˜•í™” ì´í•´
- [ ] Jacobianì˜ ì˜ë¯¸ ì´í•´
- [ ] Gauss-Newton ì•Œê³ ë¦¬ì¦˜ ì´í•´
- [ ] LMì´ GNë³´ë‹¤ ë‚˜ì€ ì  ì„¤ëª… ê°€ëŠ¥
- [ ] Ceresì˜ CostFunction, Problem ì—­í•  ì´í•´

---

## ğŸ”— Phase 1 ì™„ë£Œ!

Week 8 ì™„ë£Œ í›„ â†’ **Phase 2: ì»´í“¨í„° ë¹„ì „ ê¸°ì´ˆ**ë¡œ ì´ë™
